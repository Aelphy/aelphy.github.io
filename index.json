
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a Software Engineer at Google working on efficient on-device computing.\nI finished PhD at ETH Zürich in the Photogrammetry and Remote Sensing Lab, supervised by Prof. Dr. Konrad Schindler.\nBefore moving to Switzerland I completed my masters in Moscow where I graduated with honors from Moscow Institute of Physics and Technology (MIPT) and from Skolkovo Institute of Science and Technology (Skoltech)\nI started my expansion towards west with a move to Kazan (where I spent 1 year at Innopolis) from Ekaterinburg (where I grew up and made my first bachelor degree in electrical engineering and graduated with honors from Ural Federal University).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Software Engineer at Google working on efficient on-device computing.\nI finished PhD at ETH Zürich in the Photogrammetry and Remote Sensing Lab, supervised by Prof. Dr. Konrad Schindler.","tags":null,"title":"Misha Usvyatsov","type":"authors"},{"authors":null,"categories":null,"content":" Quickly discover relevant content by filtering publications. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3b8be3c3e99a94c71f5a4950c9057d4c","permalink":"/home/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/publications/","section":"home","summary":" Quickly discover relevant content by filtering publications. ","tags":null,"title":"Recent Publications","type":"home"},{"authors":null,"categories":null,"content":"I taught the following courses:\nTeaching assistant of Prof. Konrad Schindler.\nImage Interpretation, Fall 2017 - 2021 Zürich, Switzerland Teaching assistant of Prof. Stamatios Lefkimmiatis.\nSignal and Image Processing, Spring 2017, Moscow, Russia Course instructor.\nIntroduction to Deep Learning, May 2017, April 2021, Yerevan, Armenia Introduction to Scientific Computing, Fall 2018 - 2023 Zürich, Switzerland ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461110400,"objectID":"67b61fe6987980ae1a91d0cd5a7ef83a","permalink":"/home/teaching/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/home/teaching/","section":"home","summary":"I taught the following courses:\nTeaching assistant of Prof. Konrad Schindler.\nImage Interpretation, Fall 2017 - 2021 Zürich, Switzerland Teaching assistant of Prof. Stamatios Lefkimmiatis.\nSignal and Image Processing, Spring 2017, Moscow, Russia Course instructor.","tags":null,"title":"Teaching","type":"home"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Mikhail Usvyatsov","Rafael Ballester-Rippoll","Lina Bashaeva","Konrad Schindler","Gonzalo Ferrer","Ivan Oseledets"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665598878,"objectID":"261337bdaba4de84ea354cd664145a09","permalink":"/publication/usvyatsov-2022-t-4-dt/","publishdate":"2022-10-12T18:21:18.143148Z","relpermalink":"/publication/usvyatsov-2022-t-4-dt/","section":"publication","summary":"","tags":[],"title":"T4DT: Tensorizing Time for Learning Temporal 3D Visual Data","type":"publication"},{"authors":["Mikhail Usvyatsov","Rafael Ballester-Ripoll","Konrad Schindler"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665598878,"objectID":"f377b4a80a61c39eb1ebc104a642fb4d","permalink":"/publication/usvyatsov-2022-tntorch/","publishdate":"2022-10-12T18:21:18.422329Z","relpermalink":"/publication/usvyatsov-2022-tntorch/","section":"publication","summary":"","tags":[],"title":"tntorch: Tensor network learning with PyTorch","type":"publication"},{"authors":["Anton Obukhov","Mikhail Usvyatsov","Christos Sakaridis","Konrad Schindler","Luc Van Gool"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665598878,"objectID":"ab025da4312706f016bde0870fb03677","permalink":"/publication/obukhov-2022-tt/","publishdate":"2022-10-12T18:21:18.286656Z","relpermalink":"/publication/obukhov-2022-tt/","section":"publication","summary":"","tags":[],"title":"TT-NF: Tensor Train Neural Fields","type":"publication"},{"authors":["Mikhail Usvyatsov","Anastasia Makarova","Rafael Ballester-Ripoll","Maxim Rakhuba","Andreas Krause","Konrad Schindler"],"categories":null,"content":"","date":1627344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627344000,"objectID":"1847e2766ba9b03675989f2bb5b5c19e","permalink":"/publication/usvyatsov-2021-cpic/","publishdate":"2021-07-27T01:18:36.552293Z","relpermalink":"/publication/usvyatsov-2021-cpic/","section":"publication","summary":"An end-to-end trainable framework that processes large-scale visual data tensors by looking at a fraction of their entries only","tags":null,"title":"Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data via Differentiable Cross-Approximation","type":"publication"},{"authors":[],"categories":[],"content":"An interactive reconstruction can be found here.\n","date":1625912045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625912045,"objectID":"2908d486c88753ff7ae03ce0d8bada83","permalink":"/post/mulegns/","publishdate":"2021-07-10T11:14:05+01:00","relpermalink":"/post/mulegns/","section":"post","summary":"The story of three weeks project as a part of Geodesic Project Course.","tags":[],"title":"Large-scale 3D modeling of a village Mulegns","type":"post"},{"authors":["Shengyu Huang","Zan Gojcic","Mikhail Usvyatsov","Andreas Wieser","Konrad Schindler"],"categories":null,"content":"","date":1616112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616112000,"objectID":"d3ff59608450b3e63974ec39eacd6236","permalink":"/publication/huang-2021-predator/","publishdate":"2021-03-19T01:18:36.552293Z","relpermalink":"/publication/huang-2021-predator/","section":"publication","summary":"A model for pairwise point-cloud registration with deep attention to the overlap region.","tags":null,"title":"PREDATOR: Registration of 3D Point Clouds with Low Overlap","type":"publication"},{"authors":[],"categories":[],"content":"I backed Keychron K3 on Kickstarter, and after a long wait, I received the keyboard with brown optical switches (K3E3 modification), which is pretty awesome except for one nasty tiny detail: the keystroke registers far before the tactile bump.\nHere is the illustration of the fixing method, which was also described on reddit.\nThe first switch took me some time to understand how it works. It is not very difficult to disassemble, but losing some parts is easy. So before the start, I highly recommend placing the patient switch inside a highlighted cardboard box and keeping it there while working on it. Seriously, consider working inside the cardboard box. You might lose very tinny parts like spring or stem; otherwise, they can jump away quickly, and the box will help to contain them. Keychron brown optical switch 1. Take a look at the bottom of the switch: Bottom of K3E3 switch The red circle shows where the stem breaks the light beam when the key is pressed. We will remove it. You don’t need to open the switch yet.\n2. Press the switch. Pressed K3E3 switch The stem will move closer to the bottom of the switch.\n3. Use tweezers or another appropriate tool to press the stem out of the housing. Top of K3E3 switch Taking the stem out While taking the stem out, keep the switch in your hand and make sure that the stem will fall out in a box / safe place.\n4. Keep the stem in a safe place for a moment. Stem of K3E3 Stem and spring separated 5. Now use tweezers to open the switch. Opening of K3E3 Opened of K3E3 Please don’t move the spring you see in the open switch. They need to be left in their places. The big spring is creating the actuation force. The small spring is responsible for creating a tactile bump.\n6. Now take the stem without the spring and insert it from the bottom of the switch. Inserting the stem into K3E3 Opened K3E3 switch Then place the small spring from the inner side of the switch bottom.\n7. Now pre-insert the brown part back into the place, but do not press fully yet. We need to move the tactile spring to the correct position. Preinsertion Placing the tactile spring Full press Closing When fully pressed and tactile spring on place, secure the stem in the brown part pressing from the bottom side with tweezers. And finally, close the switch entirely. The upgrade is finished.\nThis post was typed on the upgraded K3E3. The fix took about 4 hours and improved typing speed by a factor of two, and reduced the error rate by a factor of 6.\n","date":1614248045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614248045,"objectID":"80783de95f082d2b8e277fac43f4d204","permalink":"/post/k3e3_fix/","publishdate":"2021-02-25T11:14:05+01:00","relpermalink":"/post/k3e3_fix/","section":"post","summary":"How I fixed Keychron K3 brown switch.","tags":[],"title":"Fixing Keychron K3 brown switch","type":"post"},{"authors":["Shengyu Huang","Mikhail Usvyatsov","Konrad Schindler"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"113b03cdfd16c2f2017761cefa13647c","permalink":"/publication/huang-2020-indoor/","publishdate":"2020-02-29T01:18:36.55387Z","relpermalink":"/publication/huang-2020-indoor/","section":"publication","summary":"Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating in indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way of improving scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy.","tags":null,"title":"Indoor Scene Recognition in 3D","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Mikhail Usvyatsov","Konrad Schindler"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c319fab7656d65c1d8e4028ca51310c0","permalink":"/publication/usvyatsov-2019-visual/","publishdate":"2020-02-29T01:18:36.55387Z","relpermalink":"/publication/usvyatsov-2019-visual/","section":"publication","summary":"Recognising relevant objects or object states in its environment is a basic capability for an autonomous robot. The dominant approach to object recognition in images and range images is classification by supervised machine learning, nowadays mostly with deep convolutional neural networks (CNNs). This works well for target classes whose variability can be completely covered with training examples. However, a robot moving in the wild, i.e., in an environment that is not known at the time the recognition system is trained, will often face \u001bmph{domain shift}: the training data cannot be assumed to exhaustively cover all the within-class variability that will be encountered in the test data. In that situation, learning is in principle possible, since the training set does capture the defining properties, respectively dissimilarities, of the target classes. But directly training a CNN to predict class probabilities is prone to overfitting to irrelevant correlations between the class labels and the specific subset of the target class that is represented in the training set. We explore the idea to instead learn a Siamese CNN that acts as similarity function between pairs of training examples. Class predictions are then obtained by measuring the similarities between a new test instance and the training samples. We show that the CNN embedding correctly recovers the relative similarities to arbitrary class exemplars in the training set. And that therefore few, randomly picked training exemplars are sufficient to achieve good predictions, making the procedure efficient.","tags":null,"title":"Visual recognition in the wild by sampling deep similarity functions","type":"publication"},{"authors":["Timo Hackel","Mikhail Usvyatsov","Silvano Galliani","Jan D Wegner","Konrad Schindler"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"2a801d6f57e604d800ce6c841371d88d","permalink":"/publication/hackel-2018-inference/","publishdate":"2020-02-29T01:18:36.553322Z","relpermalink":"/publication/hackel-2018-inference/","section":"publication","summary":"While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the back-propagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.","tags":null,"title":"Inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks","type":"publication"},{"authors":["Maxim Borisyak","Mikhail Usvyatsov","Michael Mulhearn","Chase Shimmin","Andrey Ustyuzhanin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2b883d6927bfb343ecb54d9784abf3a6","permalink":"/publication/borisyak-2017-muon/","publishdate":"2020-02-29T01:18:36.552293Z","relpermalink":"/publication/borisyak-2017-muon/","section":"publication","summary":"Neural architecture that allows for simultaneous optimization of computational cost with per-pixel cross-entropy loss.","tags":null,"title":"Muon trigger for mobile phones","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]